{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "# Q-Learning and SARSA Applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying SARSA to Taxi-v2\n",
    "\n",
    "    The Taxi Problem\n",
    "    \n",
    "    from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
    "    by Tom Dietterich\n",
    "    \n",
    "    Description:\n",
    "    \n",
    "    There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "    \n",
    "    Observations: \n",
    "    \n",
    "    There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n",
    "    \n",
    "    Passenger locations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    - 4: in taxi\n",
    "    \n",
    "    Destinations:\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "        \n",
    "    Actions:\n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east \n",
    "    - 3: move west \n",
    "    - 4: pickup passenger\n",
    "    - 5: dropoff passenger\n",
    "    \n",
    "    Rewards: \n",
    "    There is a default per-step reward of -1,\n",
    "    except for delivering the passenger, which is +20,\n",
    "    or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
    "    \n",
    "#### Attention: there are state agreggation, where different states become a single one. For example, the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(500)\n",
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  427\n",
      "Action taken:  2\n",
      "Next state:  447 \n",
      "Reward:  -1\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "State:  447\n",
      "Action taken:  4\n",
      "Next state:  447 \n",
      "Reward:  -10\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "\n",
      "State:  447\n",
      "Action taken:  3\n",
      "Next state:  427 \n",
      "Reward:  -1\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "steps = []\n",
    "for step in range(3):\n",
    "    \n",
    "    #plt.imshow(env.render(mode='rgb_array'))\n",
    "    #display.display(plt.gcf())\n",
    "    #display.clear_output(wait=True)\n",
    "    action = env.action_space.sample()\n",
    "    step = env.step(action)\n",
    "    next_state = env.step(action)[0]\n",
    "    print('State: ',state)\n",
    "    print('Action taken: ',action)\n",
    "    print('Next state: ',step[0],'\\nReward: ',step[1])\n",
    "    state = next_state\n",
    "    steps.append(env.render())\n",
    "    \n",
    "    print()\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 197, -1, False)],\n",
       " 1: [(1.0, 97, -1, False)],\n",
       " 2: [(1.0, 97, -1, False)],\n",
       " 3: [(1.0, 77, -1, False)],\n",
       " 4: [(1.0, 97, -10, False)],\n",
       " 5: [(1.0, 85, 20, True)]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will define the e-greedy function that will make the policy e-greedy\n",
    "\n",
    "def e_greedy(Q, s , epsilon=0.1):\n",
    "    \n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        \n",
    "        return np.random.randint(Q.shape[1])\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return np.argmax(Q[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to test the policy midway the policy learning\n",
    "\n",
    "def run_episodes(env, Q, num_episodes=100, to_print=False):\n",
    "    \n",
    "    total_rew = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        game_rew = 0\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(np.argmax(Q[state]))\n",
    "            state = next_state\n",
    "            game_rew += reward\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                total_rew.append(game_rew)\n",
    "        \n",
    "    if to_print:\n",
    "        \n",
    "        print('Mean score: %.3f of %i games!'%(np.mean(total_rew), num_episodes))\n",
    "    \n",
    "    else:\n",
    "        return np.mean(total_rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define the SARSA algorithm\n",
    "\n",
    "def SARSA(env, alpha=0.01, num_episodes=10000, epsilon=0.3, gamma=0.95, eps_decay=0.00005):\n",
    "    \n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "    test_rewards = []\n",
    "    test_episodes = []\n",
    "    Q = np.zeros((nS,nA))\n",
    "    games_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_rew = 0\n",
    "        \n",
    "        if epsilon > 0.01:\n",
    "            \n",
    "            epsilon -= eps_decay\n",
    "        \n",
    "        action = e_greedy(Q, state, epsilon)\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            next_action = e_greedy(Q, next_state, epsilon)\n",
    "            \n",
    "            Q[state,action] = Q[state,action] + alpha * (reward + gamma * Q[next_state,next_action] - Q[state,action])\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        \n",
    "            total_rew += reward\n",
    "            \n",
    "            if done:\n",
    "                games_rewards.append(total_rew)\n",
    "                \n",
    "        \n",
    "        if (episode%300) == 0:\n",
    "            \n",
    "            test_rew = run_episodes(env, Q, 1000)\n",
    "            print('Episode: {:5d} Eps: {:2.4f} Rew: {:2.4f}'.format(episode, epsilon, test_rew))\n",
    "            test_episodes.append(episode)\n",
    "            test_rewards.append(test_rew)\n",
    "            \n",
    "    \n",
    "    return Q,test_rewards,test_episodes\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:     0 Eps: 0.3990 Rew: -226.8830\n",
      "Episode:   300 Eps: 0.0990 Rew: -198.2880\n",
      "Episode:   600 Eps: 0.0100 Rew: -232.3370\n",
      "Episode:   900 Eps: 0.0100 Rew: -180.1300\n",
      "Episode:  1200 Eps: 0.0100 Rew: -109.7960\n",
      "Episode:  1500 Eps: 0.0100 Rew: -57.5300\n",
      "Episode:  1800 Eps: 0.0100 Rew: -33.0870\n",
      "Episode:  2100 Eps: 0.0100 Rew: -24.9710\n",
      "Episode:  2400 Eps: 0.0100 Rew: -8.4660\n",
      "Episode:  2700 Eps: 0.0100 Rew: -1.6060\n",
      "Episode:  3000 Eps: 0.0100 Rew: 4.5110\n",
      "Episode:  3300 Eps: 0.0100 Rew: 5.4510\n",
      "Episode:  3600 Eps: 0.0100 Rew: 6.5210\n",
      "Episode:  3900 Eps: 0.0100 Rew: 7.9590\n",
      "Episode:  4200 Eps: 0.0100 Rew: 8.0080\n",
      "Episode:  4500 Eps: 0.0100 Rew: 7.9720\n",
      "Episode:  4800 Eps: 0.0100 Rew: 7.8520\n"
     ]
    }
   ],
   "source": [
    "Q, test_rewards, test_episodes = SARSA(env, alpha=0.1, num_episodes=5000, epsilon=0.4, gamma=0.95, eps_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards = pd.DataFrame()\n",
    "df_rewards['Episodes'] = test_episodes\n",
    "df_rewards['Mean_Rewards'] = test_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-2f471b9058584f3ead4d1bf06c3f2b9d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2f471b9058584f3ead4d1bf06c3f2b9d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2f471b9058584f3ead4d1bf06c3f2b9d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0da5cfbe8a39effaf673caeb0eb9712a\"}, \"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"field\": \"Episodes\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Mean_Rewards\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-0da5cfbe8a39effaf673caeb0eb9712a\": [{\"Episodes\": 0, \"Mean_Rewards\": -226.883}, {\"Episodes\": 300, \"Mean_Rewards\": -198.288}, {\"Episodes\": 600, \"Mean_Rewards\": -232.337}, {\"Episodes\": 900, \"Mean_Rewards\": -180.13}, {\"Episodes\": 1200, \"Mean_Rewards\": -109.796}, {\"Episodes\": 1500, \"Mean_Rewards\": -57.53}, {\"Episodes\": 1800, \"Mean_Rewards\": -33.087}, {\"Episodes\": 2100, \"Mean_Rewards\": -24.971}, {\"Episodes\": 2400, \"Mean_Rewards\": -8.466}, {\"Episodes\": 2700, \"Mean_Rewards\": -1.606}, {\"Episodes\": 3000, \"Mean_Rewards\": 4.511}, {\"Episodes\": 3300, \"Mean_Rewards\": 5.451}, {\"Episodes\": 3600, \"Mean_Rewards\": 6.521}, {\"Episodes\": 3900, \"Mean_Rewards\": 7.959}, {\"Episodes\": 4200, \"Mean_Rewards\": 8.008}, {\"Episodes\": 4500, \"Mean_Rewards\": 7.972}, {\"Episodes\": 4800, \"Mean_Rewards\": 7.852}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(df_rewards).mark_line().encode(alt.X('Episodes'),alt.Y('Mean_Rewards'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spinningup)",
   "language": "python",
   "name": "spinningup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
