{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "import atari_wrappers as atari\n",
    "\n",
    "#import numba\n",
    "#from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_env(env_name, fire=True, frames_num=2, noop_num=30, skip_frames=True):\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    if skip_frames:\n",
    "        env = atari.MaxAndSkipEnv(env) ## Return only every skip-th frame\n",
    "        \n",
    "    if fire:\n",
    "        env = atari.FireResetEnv(env) ## Fire at the beggining\n",
    "        \n",
    "    env = atari.NoopResetEnv(env,noop_max=noop_num)\n",
    "    env = atari.WarpFrame(env) ## Reshape image\n",
    "    env = atari.FrameStack(env, frames_num) ## Stack last 2 frames\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class QNet(Model):\n",
    "    \n",
    "    my_strategy = tf.distribute.MirroredStrategy()\n",
    "    with my_strategy.scope():\n",
    "        @tf.function\n",
    "        def __init__(self, h_layers, h_size, o_size, h_activation=tf.nn.relu, o_activation=None):\n",
    "        \n",
    "            super(QNet,self).__init__()\n",
    "            #self.conv_layer1 = Conv2D(filters=32, kernel_size=8, strides=4, \n",
    "                                      #kernel_initializer=VarianceScaling(scale=2.),use_bias=False,\n",
    "                                      #padding='valid', activation='relu')\n",
    "            #self.conv_layer2 = Conv2D(filters=64, kernel_size=4, strides=2,\n",
    "                                      #kernel_initializer=VarianceScaling(scale=2.),use_bias=False,\n",
    "                                      #padding='valid', activation='relu')\n",
    "            #self.conv_layer3 = Conv2D(filters=64, kernel_size=3, strides=1,\n",
    "                                      #kernel_initializer=VarianceScaling(scale=2.),use_bias=False,\n",
    "                                      #padding='valid', activation='relu')\n",
    "            #self.conv_layer4 = Conv2D(filters=1024, kernel_size=7, strides=1,\n",
    "                                      #kernel_initializer=VarianceScaling(scale=2.),use_bias=False,\n",
    "                                      #padding='valid', activation='relu')\n",
    "            self.conv_layer1 = Conv2D(filters=32, kernel_size=8, strides=4, padding='valid', activation='relu')\n",
    "            self.conv_layer2 = Conv2D(filters=64, kernel_size=4, strides=2, padding='valid', activation='relu')\n",
    "            self.conv_layer3 = Conv2D(filters=64, kernel_size=3, strides=1, padding='valid', activation='relu')\n",
    "            self.conv_layer4 = Conv2D(filters=1024, kernel_size=7, strides=1, padding='valid', activation='relu')\n",
    "        \n",
    "            self.flatten_layer = Flatten()\n",
    "            \n",
    "            self.hidden_layers = [Dense(h_size[i], activation=h_activation) for i in range(h_layers)]\n",
    "            self.output_layer = Dense(o_size, activation=o_activation)\n",
    "            \n",
    "            #self.hidden_layers = [Dense(h_size[i],kernel_initializer=VarianceScaling(scale=2.),\n",
    "             #                           activation=h_activation) for i in range(h_layers)]\n",
    "            #self.output_layer = Dense(o_size, kernel_initializer=VarianceScaling(scale=2.),\n",
    "              #                        activation=o_activation)\n",
    "                \n",
    "                \n",
    "    with my_strategy.scope():\n",
    "        @tf.function\n",
    "        def call(self,input_data):\n",
    "        \n",
    "            x = input_data\n",
    "        \n",
    "            x = self.conv_layer1(x)\n",
    "            x = self.conv_layer2(x)\n",
    "            x = self.conv_layer3(x)\n",
    "            x = self.conv_layer4(x)\n",
    "        \n",
    "            x = self.flatten_layer(x)\n",
    "        \n",
    "            for layer in self.hidden_layers:\n",
    "            \n",
    "                x = layer(x)\n",
    "            \n",
    "            return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_frames(frames):\n",
    "    \n",
    "    return np.array(frames, dtype=np.float32)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceBuffer():\n",
    "    \n",
    "    def __init__(self,buffer_size):\n",
    "        \n",
    "        self.obs_buf = deque(maxlen=buffer_size)\n",
    "        self.rew_buf = deque(maxlen=buffer_size)\n",
    "        self.act_buf = deque(maxlen=buffer_size)\n",
    "        self.next_obs_buf = deque(maxlen=buffer_size)\n",
    "        self.done_buf = deque(maxlen=buffer_size)\n",
    "        \n",
    "    def add(self, obs, rew, act, next_obs, done):\n",
    "        \n",
    "        self.obs_buf.append(obs)\n",
    "        self.rew_buf.append(rew)\n",
    "        self.act_buf.append(act)\n",
    "        self.next_obs_buf.append(next_obs)\n",
    "        self.done_buf.append(done)\n",
    "        \n",
    "    def sample_minibatch(self, batch_size):\n",
    "        \n",
    "        mb_indices = np.random.randint(len(self.obs_buf),size=batch_size)\n",
    "        \n",
    "        mb_obs = scale_frames([self.obs_buf[i] for i in mb_indices])\n",
    "        mb_rew = [self.rew_buf[i] for i in mb_indices] \n",
    "        mb_act = [self.act_buf[i] for i in mb_indices]\n",
    "        mb_next_obs = scale_frames([self.next_obs_buf[i] for i in mb_indices])\n",
    "        mb_done = [self.done_buf[i] for i in mb_indices]\n",
    "    \n",
    "        return mb_obs, mb_rew, mb_act, mb_next_obs, mb_done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.obs_buf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_milli_time = lambda: int(round(time.time() * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(target_qv,online_qv):\n",
    "    \n",
    "    target_qv.set_weights(online_qv.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(action_values,epsilon=0.1):\n",
    "    \n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        \n",
    "        return np.random.randint(len(action_values))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return np.argmax(action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the target value for each item in the mini_batch, that will be used in the loss function\n",
    "\n",
    "def q_target_values(mini_batch_rewards, mini_batch_done, action_values, \n",
    "                    gamma, DQN_variation, one_hot_actions): \n",
    "    \n",
    "    max_action_value = np.max(action_values, axis=1) #DQN\n",
    "    Q_target = tf.reduce_sum(tf.multiply(action_values,one_hot_actions),axis=1) #DDQN\n",
    "    \n",
    "    ys = []\n",
    "    for reward, done ,action_value, ddqn_target_v in zip(mini_batch_rewards, \n",
    "                                          mini_batch_done, max_action_value, Q_target):    \n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            ys.append(reward)\n",
    "        \n",
    "        else:\n",
    "            if DQN_variation == 'DQN':\n",
    "                ys.append(reward + gamma * action_value)\n",
    "            \n",
    "            elif DQN_variation == 'DDQN':\n",
    "                \n",
    "                ys.append(reward + gamma * ddqn_target_v)\n",
    "            \n",
    "    assert len(ys) ==  len(mini_batch_rewards)\n",
    "    \n",
    "    return np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env_test, online_qv, num_games=20):\n",
    "    \n",
    "    games_rewards = []\n",
    "    \n",
    "    for _ in range(num_games):\n",
    "        \n",
    "        done = False\n",
    "        g_reward = 0\n",
    "        obs = env_test.reset()\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            obs_process = np.array([scale_frames(obs)])\n",
    "            action_values = online_qv.predict(obs_process)[0]\n",
    "            \n",
    "            action = e_greedy(action_values, epsilon=0.05)\n",
    "            #action = np.argmax(action_values)\n",
    "            \n",
    "            next_obs, reward, done, _ = env_test.step(action)\n",
    "            \n",
    "            obs = next_obs\n",
    "            \n",
    "            g_reward += reward\n",
    "            \n",
    "        games_rewards.append(g_reward)\n",
    "        \n",
    "    return games_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN(env_name, hidden_layers =1, hidden_size=[32], alpha=1e-2, num_epochs=2000, buffer_size=100000, gamma=0.99,\n",
    "        update_target_net=1000, batch_size=64, update_freq=4, frames_num=2, min_buffer_size=5000, test_frequency=20,\n",
    "        start_exp=1, end_exp=0.1, exp_steps=100000, render_cycle=100, DQN_variation = 'DQN'):\n",
    "    \n",
    "    #checkpoint_path = 'training_1/cp_dqn.ckpt'\n",
    "    #checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    #cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                     #save_weights_only=True,\n",
    "                                                     #verbose=0)\n",
    "    \n",
    "    env = make_env(env_name, frames_num=frames_num, skip_frames=True, noop_num=20)\n",
    "    env_test = make_env(env_name,frames_num=frames_num, skip_frames=True, noop_num=20)\n",
    "    \n",
    "    env_test = gym.wrappers.Monitor(env_test, \"VIDEOS/TEST_VIDEOS\"+env_name+str(current_milli_time()), force=True,\n",
    "                                    video_callable=lambda x: x%20==0)\n",
    "    \n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.n\n",
    "    \n",
    "    target_qv = QNet(h_layers=hidden_layers, h_size=hidden_size, o_size=act_dim)\n",
    "    online_qv = QNet(h_layers=hidden_layers, h_size=hidden_size, o_size=act_dim)\n",
    "    \n",
    "    obs = env.reset()\n",
    "    obs = scale_frames(obs)\n",
    "    \n",
    "    _ = target_qv.predict(np.array([obs]))\n",
    "    _ = online_qv.predict(np.array([obs]))\n",
    "    \n",
    "    online_qv.compile(optimizer = tf.keras.optimizers.Adam(alpha),\n",
    "                      loss = tf.keras.losses.MeanSquaredError())\n",
    "    \n",
    "    target_qv.compile(optimizer = tf.keras.optimizers.Adam(alpha),\n",
    "                     loss = tf.keras.losses.MeanSquaredError())\n",
    "    \n",
    "    update_target(target_qv,online_qv)\n",
    "    \n",
    "    #####################\n",
    "    ### TENSORBOARD ##### --> Not implemented\n",
    "    #####################\n",
    "    \n",
    "    render_the_game = False\n",
    "    step_count = 0\n",
    "    last_update_loss = []\n",
    "    mean_loss = []\n",
    "    mean_reward_test = []\n",
    "    steps_test = []\n",
    "    ep_time = current_milli_time()\n",
    "    batch_rew = []\n",
    "    old_step_count = 0\n",
    "    \n",
    "    buffer = ExperienceBuffer(buffer_size)\n",
    "    epsilon = start_exp\n",
    "    eps_decay = (start_exp - end_exp)/exp_steps\n",
    "    \n",
    "    obs = env.reset()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        game_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            obs_process = np.array([scale_frames(obs)])\n",
    "            action_values = online_qv.predict(obs_process)[0]\n",
    "            \n",
    "            action = e_greedy(action_values, epsilon)\n",
    "            next_obs, reward, done, _ = env.step(action) \n",
    "            \n",
    "            if render_the_game:\n",
    "                env.render()\n",
    "            \n",
    "            buffer.add(obs, reward, action, next_obs, done)\n",
    "            \n",
    "            obs = next_obs\n",
    "            game_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            if epsilon > end_exp:\n",
    "                epsilon -= eps_decay\n",
    "                \n",
    "            if len(buffer) > min_buffer_size and (step_count % update_freq == 0):\n",
    "                \n",
    "                mb_obs, mb_reward, mb_action, mb_next_obs, mb_done = buffer.sample_minibatch(batch_size)\n",
    "                mb_target_actions = target_qv.predict(mb_next_obs)\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    q_values = online_qv(mb_obs)\n",
    "                    \n",
    "                    one_hot_actions = tf.keras.utils.to_categorical(mb_action,act_dim,dtype=np.float32)\n",
    "                    Q = tf.reduce_sum(tf.multiply(q_values,one_hot_actions),axis=1)\n",
    "                    \n",
    "                    mini_batch_y = q_target_values(mb_reward, mb_done, \n",
    "                                                   mb_target_actions, \n",
    "                                                   gamma, DQN_variation,one_hot_actions)\n",
    "                       \n",
    "                    \n",
    "                    error = Q - mini_batch_y\n",
    "                    \n",
    "                    loss = tf.keras.losses.Huber()(mini_batch_y, Q)\n",
    "                    \n",
    "                    model_gradients = tape.gradient(loss, online_qv.trainable_variables)\n",
    "                    online_qv.optimizer.apply_gradients(zip(model_gradients, online_qv.trainable_variables))\n",
    "                    \n",
    "                    last_update_loss.append(loss)\n",
    "                \n",
    "                # target update\n",
    "                \n",
    "            if (len(buffer) > min_buffer_size) and (step_count % update_target_net == 0):\n",
    "                    \n",
    "                mean_loss.append(np.mean(last_update_loss))\n",
    "                last_update_loss = []\n",
    "                    \n",
    "                update_target(target_qv,online_qv)\n",
    "                \n",
    "            if done:\n",
    "                \n",
    "                obs = env.reset()\n",
    "                batch_rew.append(game_reward)\n",
    "                game_reward = 0\n",
    "                render_the_game = False\n",
    "            \n",
    "        \n",
    "        if epoch % test_frequency == 0:\n",
    "            #start = time.time()\n",
    "            test_reward = test_agent(env_test, online_qv, num_games=10)\n",
    "            \n",
    "            ep_sec_time = int((current_milli_time()-ep_time) / 1000)\n",
    "            print('Epoch:%4d Reward:%4.2f, Epsilon:%2.2f,  Step:%5d,   Test (mean),(std):(%4.2f), (%4.2f),   Time:%d,   Epoch_Steps:%d' %\n",
    "                  (epoch,np.mean(batch_rew), epsilon, step_count, np.mean(test_reward), np.std(test_reward), \n",
    "                   ep_sec_time, (step_count-old_step_count)/test_frequency))\n",
    "            \n",
    "            ep_time = current_milli_time()\n",
    "            batch_rew = []\n",
    "            old_step_count = step_count  \n",
    "            #print('\\nTest: ',time.time()-start,'\\n')\n",
    "            \n",
    "            if DQN_variation == 'DQN':\n",
    "                online_qv.save_weights('./saved_models/dqn_pong')\n",
    "            \n",
    "            elif DQN_variation == 'DDQN':\n",
    "                online_qv.save_weights('./saved_models/ddqn_pong')\n",
    "            \n",
    "            mean_reward_test.append(np.mean(test_reward))\n",
    "            steps_test.append(step_count)\n",
    "            \n",
    "        if epoch % render_cycle == 0:\n",
    "            render_the_game = True\n",
    "                \n",
    "    env.close()\n",
    "    \n",
    "    return env_test, online_qv, mean_reward_test, steps_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 Reward:-20.00, Epsilon:0.99,  Step: 1039,   Test (mean),(std):(-21.00), (0.00),   Time:136,   Epoch_Steps:51\n",
      "Epoch:  20 Reward:-20.60, Epsilon:0.83,  Step:18988,   Test (mean),(std):(-20.80), (0.40),   Time:699,   Epoch_Steps:897\n",
      "Epoch:  40 Reward:-20.80, Epsilon:0.68,  Step:35784,   Test (mean),(std):(-21.00), (0.00),   Time:657,   Epoch_Steps:839\n",
      "Epoch:  60 Reward:-20.85, Epsilon:0.53,  Step:52555,   Test (mean),(std):(-21.00), (0.00),   Time:661,   Epoch_Steps:838\n",
      "Epoch:  80 Reward:-18.80, Epsilon:0.30,  Step:78324,   Test (mean),(std):(-20.90), (0.30),   Time:948,   Epoch_Steps:1288\n",
      "Epoch: 100 Reward:-17.75, Epsilon:0.10,  Step:103309,   Test (mean),(std):(-16.40), (1.43),   Time:1035,   Epoch_Steps:1249\n",
      "Epoch: 120 Reward:-15.55, Epsilon:0.10,  Step:133807,   Test (mean),(std):(-13.60), (2.15),   Time:1287,   Epoch_Steps:1524\n",
      "Epoch: 140 Reward:-15.45, Epsilon:0.10,  Step:166982,   Test (mean),(std):(-18.50), (3.72),   Time:1257,   Epoch_Steps:1658\n",
      "Epoch: 160 Reward:-20.60, Epsilon:0.10,  Step:184035,   Test (mean),(std):(-21.00), (0.00),   Time:685,   Epoch_Steps:852\n",
      "Epoch: 180 Reward:-20.75, Epsilon:0.10,  Step:200964,   Test (mean),(std):(-20.60), (0.49),   Time:681,   Epoch_Steps:846\n"
     ]
    }
   ],
   "source": [
    "env_ddqn, online_qv_ddqn, mean_reward_test_ddqn, steps_test_ddqn= DQN('PongNoFrameskip-v4', hidden_layers =1, hidden_size=[128], alpha=2e-4, num_epochs=200, \n",
    "                buffer_size=100000, gamma=0.99, update_target_net=1000, batch_size=32, update_freq=2, \n",
    "                frames_num=2, min_buffer_size=1000, test_frequency=20, start_exp=1, end_exp=0.1, \n",
    "                exp_steps=100000, render_cycle=10000, DQN_variation = 'DDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 Reward:-21.00, Epsilon:0.99,  Step:  876,   Test (mean),(std):(-20.60), (0.49),   Time:130,   Epoch_Steps:43\n",
      "Epoch:  20 Reward:-20.05, Epsilon:0.83,  Step:19342,   Test (mean),(std):(-21.00), (0.00),   Time:669,   Epoch_Steps:923\n",
      "Epoch:  40 Reward:-20.00, Epsilon:0.65,  Step:38654,   Test (mean),(std):(-20.80), (0.60),   Time:711,   Epoch_Steps:965\n",
      "Epoch:  60 Reward:-18.85, Epsilon:0.43,  Step:63579,   Test (mean),(std):(-20.30), (1.10),   Time:984,   Epoch_Steps:1246\n",
      "Epoch:  80 Reward:-18.00, Epsilon:0.10,  Step:99780,   Test (mean),(std):(-16.70), (3.29),   Time:1458,   Epoch_Steps:1810\n",
      "Epoch: 100 Reward:-16.25, Epsilon:0.10,  Step:143524,   Test (mean),(std):(-11.10), (4.06),   Time:1830,   Epoch_Steps:2187\n",
      "Epoch: 120 Reward:-14.00, Epsilon:0.10,  Step:192426,   Test (mean),(std):(-13.10), (4.66),   Time:2012,   Epoch_Steps:2445\n",
      "Epoch: 140 Reward:-10.80, Epsilon:0.10,  Step:249423,   Test (mean),(std):(-4.20), (3.76),   Time:2441,   Epoch_Steps:2849\n",
      "Epoch: 160 Reward:-0.65, Epsilon:0.10,  Step:314203,   Test (mean),(std):(-5.00), (12.96),   Time:2309,   Epoch_Steps:3239\n",
      "Epoch: 180 Reward:9.50, Epsilon:0.10,  Step:370166,   Test (mean),(std):(-0.10), (13.91),   Time:2047,   Epoch_Steps:2798\n"
     ]
    }
   ],
   "source": [
    "env, online_qv, mean_reward_test, steps_test = DQN('PongNoFrameskip-v4', hidden_layers =1, hidden_size=[128], alpha=2e-4, num_epochs=200, \n",
    "                buffer_size=100000, gamma=0.99, update_target_net=1000, batch_size=32, update_freq=2, \n",
    "                frames_num=2, min_buffer_size=1000, test_frequency=20, start_exp=1, end_exp=0.1, \n",
    "                exp_steps=100000, render_cycle=10000, DQN_variation = 'DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
