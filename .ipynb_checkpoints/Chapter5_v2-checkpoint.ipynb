{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "import atari_wrappers as atari\n",
    "\n",
    "#import numba\n",
    "#from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_env(env_name, fire=True, frames_num=2, noop_num=30, skip_frames=True):\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    if skip_frames:\n",
    "        env = atari.MaxAndSkipEnv(env) ## Return only every skip-th frame\n",
    "        \n",
    "    if fire:\n",
    "        env = atari.FireResetEnv(env) ## Fire at the beggining\n",
    "        \n",
    "    env = atari.NoopResetEnv(env,noop_max=noop_num)\n",
    "    env = atari.WarpFrame(env) ## Reshape image\n",
    "    env = atari.FrameStack(env, frames_num) ## Stack last 2 frames\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class QNet(Model):\n",
    "    \n",
    "    my_strategy = tf.distribute.MirroredStrategy()\n",
    "    with my_strategy.scope():\n",
    "        @tf.function\n",
    "        def __init__(self, h_layers, h_size, o_size, h_activation=tf.nn.relu, o_activation=None):\n",
    "        \n",
    "            super(QNet,self).__init__()\n",
    "            #self.conv_layer1 = Conv2D(filters=32, kernel_size=8, strides=4, \n",
    "                                      #kernel_initializer=VarianceScaling(scale=2.),use_bias=False,\n",
    "                                      #padding='valid', activation='relu')\n",
    "            #self.conv_layer2 = Conv2D(filters=64, kernel_size=4, strides=2,\n",
    "                                      #kernel_initializer=VarianceScaling(scale=2.),use_bias=False,\n",
    "                                      #padding='valid', activation='relu')\n",
    "            #self.conv_layer3 = Conv2D(filters=64, kernel_size=3, strides=1,\n",
    "                                      #kernel_initializer=VarianceScaling(scale=2.),use_bias=False,\n",
    "                                      #padding='valid', activation='relu')\n",
    "            #self.conv_layer4 = Conv2D(filters=1024, kernel_size=7, strides=1,\n",
    "                                      #kernel_initializer=VarianceScaling(scale=2.),use_bias=False,\n",
    "                                      #padding='valid', activation='relu')\n",
    "            self.conv_layer1 = Conv2D(filters=32, kernel_size=8, strides=4, padding='valid', activation='relu')\n",
    "            self.conv_layer2 = Conv2D(filters=64, kernel_size=4, strides=2, padding='valid', activation='relu')\n",
    "            self.conv_layer3 = Conv2D(filters=64, kernel_size=3, strides=1, padding='valid', activation='relu')\n",
    "            self.conv_layer4 = Conv2D(filters=1024, kernel_size=7, strides=1, padding='valid', activation='relu')\n",
    "        \n",
    "            self.flatten_layer = Flatten()\n",
    "            \n",
    "            self.hidden_layers = [Dense(h_size[i], activation=h_activation) for i in range(h_layers)]\n",
    "            self.output_layer = Dense(o_size, activation=o_activation)\n",
    "            \n",
    "            #self.hidden_layers = [Dense(h_size[i],kernel_initializer=VarianceScaling(scale=2.),\n",
    "             #                           activation=h_activation) for i in range(h_layers)]\n",
    "            #self.output_layer = Dense(o_size, kernel_initializer=VarianceScaling(scale=2.),\n",
    "              #                        activation=o_activation)\n",
    "                \n",
    "                \n",
    "    with my_strategy.scope():\n",
    "        @tf.function\n",
    "        def call(self,input_data):\n",
    "        \n",
    "            x = input_data\n",
    "        \n",
    "            x = self.conv_layer1(x)\n",
    "            x = self.conv_layer2(x)\n",
    "            x = self.conv_layer3(x)\n",
    "            x = self.conv_layer4(x)\n",
    "        \n",
    "            x = self.flatten_layer(x)\n",
    "        \n",
    "            for layer in self.hidden_layers:\n",
    "            \n",
    "                x = layer(x)\n",
    "            \n",
    "            return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_frames(frames):\n",
    "    \n",
    "    return np.array(frames, dtype=np.float32)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceBuffer():\n",
    "    \n",
    "    def __init__(self,buffer_size):\n",
    "        \n",
    "        self.obs_buf = deque(maxlen=buffer_size)\n",
    "        self.rew_buf = deque(maxlen=buffer_size)\n",
    "        self.act_buf = deque(maxlen=buffer_size)\n",
    "        self.next_obs_buf = deque(maxlen=buffer_size)\n",
    "        self.done_buf = deque(maxlen=buffer_size)\n",
    "        \n",
    "    def add(self, obs, rew, act, next_obs, done):\n",
    "        \n",
    "        self.obs_buf.append(obs)\n",
    "        self.rew_buf.append(rew)\n",
    "        self.act_buf.append(act)\n",
    "        self.next_obs_buf.append(next_obs)\n",
    "        self.done_buf.append(done)\n",
    "        \n",
    "    def sample_minibatch(self, batch_size):\n",
    "        \n",
    "        mb_indices = np.random.randint(len(self.obs_buf),size=batch_size)\n",
    "        \n",
    "        mb_obs = scale_frames([self.obs_buf[i] for i in mb_indices])\n",
    "        mb_rew = [self.rew_buf[i] for i in mb_indices] \n",
    "        mb_act = [self.act_buf[i] for i in mb_indices]\n",
    "        mb_next_obs = scale_frames([self.next_obs_buf[i] for i in mb_indices])\n",
    "        mb_done = [self.done_buf[i] for i in mb_indices]\n",
    "    \n",
    "        return mb_obs, mb_rew, mb_act, mb_next_obs, mb_done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.obs_buf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_milli_time = lambda: int(round(time.time() * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(target_qv,online_qv):\n",
    "    \n",
    "    target_qv.set_weights(online_qv.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(action_values,epsilon=0.1):\n",
    "    \n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        \n",
    "        return np.random.randint(len(action_values))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return np.argmax(action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the target value for each item in the mini_batch, that will be used in the loss function\n",
    "\n",
    "def q_target_values(mini_batch_rewards, mini_batch_done, action_values, \n",
    "                    gamma, DQN_variation, mb_next_obs, act_dim,online_qv): \n",
    "    \n",
    "    max_action_value = np.max(action_values, axis=1) #DQN\n",
    "    \n",
    "    if DQN_variation == 'DDQN':\n",
    "        \n",
    "        q_values = online_qv(mb_next_obs)\n",
    "        max_actions_online = np.argmax(q_values, axis=1) \n",
    "        #print('\\n',max_actions_online)\n",
    "        one_hot_actions = tf.keras.utils.to_categorical(max_actions_online,act_dim,dtype=np.float32)\n",
    "        Qtarget_onlineaction = tf.reduce_sum(tf.multiply(action_values,one_hot_actions),axis=1)\n",
    "    \n",
    "    ys = []\n",
    "    for reward, done ,action_value, ddqn_target_v in zip(mini_batch_rewards, \n",
    "                                          mini_batch_done, max_action_value, Qtarget_onlineaction):    \n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            ys.append(reward)\n",
    "        \n",
    "        else:\n",
    "            if DQN_variation == 'DQN':\n",
    "                ys.append(reward + gamma * action_value)\n",
    "            \n",
    "            elif DQN_variation == 'DDQN':\n",
    "                \n",
    "                ys.append(reward + gamma * ddqn_target_v)\n",
    "            \n",
    "    assert len(ys) ==  len(mini_batch_rewards)\n",
    "    \n",
    "    return np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env_test, online_qv, num_games=20):\n",
    "    \n",
    "    games_rewards = []\n",
    "    \n",
    "    for _ in range(num_games):\n",
    "        \n",
    "        done = False\n",
    "        g_reward = 0\n",
    "        obs = env_test.reset()\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            obs_process = np.array([scale_frames(obs)])\n",
    "            action_values = online_qv.predict(obs_process)[0]\n",
    "            \n",
    "            action = e_greedy(action_values, epsilon=0.05)\n",
    "            #action = np.argmax(action_values)\n",
    "            \n",
    "            next_obs, reward, done, _ = env_test.step(action)\n",
    "            \n",
    "            obs = next_obs\n",
    "            \n",
    "            g_reward += reward\n",
    "            \n",
    "        games_rewards.append(g_reward)\n",
    "        \n",
    "    return games_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN(env_name, hidden_layers =1, hidden_size=[32], alpha=1e-2, num_epochs=2000, buffer_size=100000, gamma=0.99,\n",
    "        update_target_net=1000, batch_size=64, update_freq=4, frames_num=2, min_buffer_size=5000, test_frequency=20,\n",
    "        start_exp=1, end_exp=0.1, exp_steps=100000, render_cycle=100, DQN_variation = 'DQN'):\n",
    "    \n",
    "    #checkpoint_path = 'training_1/cp_dqn.ckpt'\n",
    "    #checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    #cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                     #save_weights_only=True,\n",
    "                                                     #verbose=0)\n",
    "    \n",
    "    env = make_env(env_name, frames_num=frames_num, skip_frames=True, noop_num=20)\n",
    "    env_test = make_env(env_name,frames_num=frames_num, skip_frames=True, noop_num=20)\n",
    "    \n",
    "    env_test = gym.wrappers.Monitor(env_test, \"VIDEOS/TEST_VIDEOS\"+env_name+str(current_milli_time()), force=True,\n",
    "                                    video_callable=lambda x: x%20==0)\n",
    "    \n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.n\n",
    "    \n",
    "    target_qv = QNet(h_layers=hidden_layers, h_size=hidden_size, o_size=act_dim)\n",
    "    online_qv = QNet(h_layers=hidden_layers, h_size=hidden_size, o_size=act_dim)\n",
    "    \n",
    "    obs = env.reset()\n",
    "    obs = scale_frames(obs)\n",
    "    \n",
    "    _ = target_qv.predict(np.array([obs]))\n",
    "    _ = online_qv.predict(np.array([obs]))\n",
    "    \n",
    "    online_qv.compile(optimizer = tf.keras.optimizers.Adam(alpha),\n",
    "                      loss = tf.keras.losses.MeanSquaredError())\n",
    "    \n",
    "    target_qv.compile(optimizer = tf.keras.optimizers.Adam(alpha),\n",
    "                     loss = tf.keras.losses.MeanSquaredError())\n",
    "    \n",
    "    update_target(target_qv,online_qv)\n",
    "    \n",
    "    #####################\n",
    "    ### TENSORBOARD ##### --> Not implemented\n",
    "    #####################\n",
    "    \n",
    "    render_the_game = False\n",
    "    step_count = 0\n",
    "    last_update_loss = []\n",
    "    mean_loss = []\n",
    "    mean_reward_test = []\n",
    "    steps_test = []\n",
    "    ep_time = current_milli_time()\n",
    "    batch_rew = []\n",
    "    old_step_count = 0\n",
    "    \n",
    "    buffer = ExperienceBuffer(buffer_size)\n",
    "    epsilon = start_exp\n",
    "    eps_decay = (start_exp - end_exp)/exp_steps\n",
    "    \n",
    "    obs = env.reset()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        game_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            obs_process = np.array([scale_frames(obs)])\n",
    "            action_values = online_qv.predict(obs_process)[0]\n",
    "            \n",
    "            action = e_greedy(action_values, epsilon)\n",
    "            next_obs, reward, done, _ = env.step(action) \n",
    "            \n",
    "            if render_the_game:\n",
    "                env.render()\n",
    "            \n",
    "            buffer.add(obs, reward, action, next_obs, done)\n",
    "            \n",
    "            obs = next_obs\n",
    "            game_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            if epsilon > end_exp:\n",
    "                epsilon -= eps_decay\n",
    "                \n",
    "            if len(buffer) > min_buffer_size and (step_count % update_freq == 0):\n",
    "                \n",
    "                mb_obs, mb_reward, mb_action, mb_next_obs, mb_done = buffer.sample_minibatch(batch_size)\n",
    "                mb_target_actions = target_qv.predict(mb_next_obs)\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    q_values = online_qv(mb_obs)\n",
    "                    \n",
    "                    one_hot_actions = tf.keras.utils.to_categorical(mb_action,act_dim,dtype=np.float32)\n",
    "                    Q = tf.reduce_sum(tf.multiply(q_values,one_hot_actions),axis=1)\n",
    "                    \n",
    "                    mini_batch_y = q_target_values(mb_reward, mb_done, \n",
    "                                                   mb_target_actions, \n",
    "                                                   gamma, DQN_variation,mb_next_obs,act_dim, online_qv)\n",
    "                       \n",
    "                    \n",
    "                    error = Q - mini_batch_y\n",
    "                    \n",
    "                    loss = tf.keras.losses.Huber()(mini_batch_y, Q)\n",
    "                    \n",
    "                    model_gradients = tape.gradient(loss, online_qv.trainable_variables)\n",
    "                    online_qv.optimizer.apply_gradients(zip(model_gradients, online_qv.trainable_variables))\n",
    "                    \n",
    "                    last_update_loss.append(loss)\n",
    "                \n",
    "                # target update\n",
    "                \n",
    "            if (len(buffer) > min_buffer_size) and (step_count % update_target_net == 0):\n",
    "                    \n",
    "                mean_loss.append(np.mean(last_update_loss))\n",
    "                last_update_loss = []\n",
    "                    \n",
    "                update_target(target_qv,online_qv)\n",
    "                \n",
    "            if done:\n",
    "                \n",
    "                obs = env.reset()\n",
    "                batch_rew.append(game_reward)\n",
    "                game_reward = 0\n",
    "                render_the_game = False\n",
    "            \n",
    "        \n",
    "        if epoch % test_frequency == 0:\n",
    "            #start = time.time()\n",
    "            test_reward = test_agent(env_test, online_qv, num_games=10)\n",
    "            \n",
    "            ep_sec_time = int((current_milli_time()-ep_time) / 1000)\n",
    "            print('Epoch:%4d Reward:%4.2f, Epsilon:%2.2f,  Step:%5d,   Test (mean),(std):(%4.2f), (%4.2f),   Time:%d,   Epoch_Steps:%d' %\n",
    "                  (epoch,np.mean(batch_rew), epsilon, step_count, np.mean(test_reward), np.std(test_reward), \n",
    "                   ep_sec_time, (step_count-old_step_count)/test_frequency))\n",
    "            \n",
    "            ep_time = current_milli_time()\n",
    "            batch_rew = []\n",
    "            old_step_count = step_count  \n",
    "            #print('\\nTest: ',time.time()-start,'\\n')\n",
    "            \n",
    "            if DQN_variation == 'DQN':\n",
    "                online_qv.save_weights('./saved_models/dqn_pong')\n",
    "            \n",
    "            elif DQN_variation == 'DDQN':\n",
    "                online_qv.save_weights('./saved_models/ddqn_pong')\n",
    "            \n",
    "            mean_reward_test.append(np.mean(test_reward))\n",
    "            steps_test.append(step_count)\n",
    "            \n",
    "        if epoch % render_cycle == 0:\n",
    "            render_the_game = True\n",
    "                \n",
    "    env.close()\n",
    "    \n",
    "    return env_test, online_qv, mean_reward_test, steps_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 Reward:-21.00, Epsilon:0.99,  Step: 1016,   Test (mean),(std):(-21.00), (0.00),   Time:132,   Epoch_Steps:50\n",
      "Epoch:  20 Reward:-20.25, Epsilon:0.82,  Step:20170,   Test (mean),(std):(-20.20), (0.40),   Time:758,   Epoch_Steps:957\n",
      "Epoch:  40 Reward:-20.20, Epsilon:0.64,  Step:40150,   Test (mean),(std):(-20.10), (0.94),   Time:808,   Epoch_Steps:999\n",
      "Epoch:  60 Reward:-18.50, Epsilon:0.40,  Step:67111,   Test (mean),(std):(-17.20), (2.04),   Time:1289,   Epoch_Steps:1348\n",
      "Epoch:  80 Reward:-16.20, Epsilon:0.10,  Step:107693,   Test (mean),(std):(-14.40), (2.46),   Time:1799,   Epoch_Steps:2029\n",
      "Epoch: 100 Reward:-9.85, Epsilon:0.10,  Step:168292,   Test (mean),(std):(-10.90), (2.77),   Time:2882,   Epoch_Steps:3029\n",
      "Epoch: 120 Reward:-8.15, Epsilon:0.10,  Step:232755,   Test (mean),(std):(2.60), (7.50),   Time:2990,   Epoch_Steps:3223\n",
      "Epoch: 140 Reward:-4.95, Epsilon:0.10,  Step:297838,   Test (mean),(std):(13.30), (5.68),   Time:2653,   Epoch_Steps:3254\n",
      "Epoch: 160 Reward:2.35, Epsilon:0.10,  Step:367006,   Test (mean),(std):(11.80), (5.76),   Time:2775,   Epoch_Steps:3458\n",
      "Epoch: 180 Reward:4.30, Epsilon:0.10,  Step:428811,   Test (mean),(std):(14.70), (2.87),   Time:2609,   Epoch_Steps:3090\n"
     ]
    }
   ],
   "source": [
    "env_ddqn, online_qv_ddqn, mean_reward_test_ddqn, steps_test_ddqn= DQN('PongNoFrameskip-v4', hidden_layers =1, hidden_size=[128], alpha=2e-4, num_epochs=200, \n",
    "                buffer_size=100000, gamma=0.99, update_target_net=1000, batch_size=32, update_freq=2, \n",
    "                frames_num=2, min_buffer_size=1000, test_frequency=20, start_exp=1, end_exp=0.1, \n",
    "                exp_steps=100000, render_cycle=10000, DQN_variation = 'DDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-bc782c46c5d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m env, online_qv, mean_reward_test, steps_test = DQN('PongNoFrameskip-v4', hidden_layers =1, hidden_size=[128], alpha=2e-4, num_epochs=200, \n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_target_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mframes_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_buffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_exp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_exp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 exp_steps=100000, render_cycle=10000, DQN_variation = 'DQN')\n",
      "\u001b[0;32m<ipython-input-31-fe0514dc0e15>\u001b[0m in \u001b[0;36mDQN\u001b[0;34m(env_name, hidden_layers, hidden_size, alpha, num_epochs, buffer_size, gamma, update_target_net, batch_size, update_freq, frames_num, min_buffer_size, test_frequency, start_exp, end_exp, exp_steps, render_cycle, DQN_variation)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m#start = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mtest_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monline_qv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mep_sec_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_milli_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mep_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-084c4afb1cad>\u001b[0m in \u001b[0;36mtest_agent\u001b[0;34m(env_test, online_qv, num_games)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m#action = np.argmax(action_values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/Reinforcement_Learning_Algorithms/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/Reinforcement_Learning_Algorithms/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLazyFrames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/Reinforcement_Learning_Algorithms/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, ac)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/Reinforcement_Learning_Algorithms/atari_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ale.lives\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[0;34m(self, screen_data)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env, online_qv, mean_reward_test, steps_test = DQN('PongNoFrameskip-v4', hidden_layers =1, hidden_size=[128], alpha=2e-4, num_epochs=200, \n",
    "                buffer_size=100000, gamma=0.99, update_target_net=1000, batch_size=32, update_freq=2, \n",
    "                frames_num=2, min_buffer_size=1000, test_frequency=20, start_exp=1, end_exp=0.1, \n",
    "                exp_steps=100000, render_cycle=10000, DQN_variation = 'DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_ddqn.close()\n",
    "obs = env_ddqn.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "            \n",
    "    obs_process = np.array([scale_frames(obs)])\n",
    "    action_values = online_qv_ddqn.predict(obs_process)[0]\n",
    "            \n",
    "    action = e_greedy(action_values, 0.05)\n",
    "    next_obs, reward, done, _ = env_ddqn.step(action) \n",
    "            \n",
    "    env_ddqn.render()\n",
    "            \n",
    "    obs = next_obs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-21.0, -20.2, -20.1, -17.2, -14.4, -10.9, 2.6, 13.3, 11.8, 14.7]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward_test_ddqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1016, 20170, 40150, 67111, 107693, 168292, 232755, 297838, 367006, 428811]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_test_ddqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
