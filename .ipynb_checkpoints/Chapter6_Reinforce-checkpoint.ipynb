{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(rewards, gamma):\n",
    "    \n",
    "    rtg = np.zeros_like(rewards , dtype=np.float32)\n",
    "    rtg[-1] = rewards[-1]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        \n",
    "        rtg[i] = rewards[i] + gamma * rtg[i+1]\n",
    "        \n",
    "    return rtg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(Model):\n",
    "    \n",
    "    def __init__(self, hidden_layers, hidden_size, output_size, activation, output_activation):\n",
    "        \n",
    "        super(Policy, self).__init__()\n",
    "        self.hidden_layers = [Dense(hidden_size[i], activation=activation) for i in range(hidden_layers)]\n",
    "        self.output_layer = Dense(output_size, activation=output_activation)\n",
    "        \n",
    "    def call(self, state):\n",
    "        \n",
    "        x = state\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            \n",
    "            x = layer(x)\n",
    "        \n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer():\n",
    "    \n",
    "    def __init__(self, gamma):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.returns = []\n",
    "    \n",
    "    def store(self, temp_traj):\n",
    "        \n",
    "        if len(temp_traj) > 0:\n",
    "            self.obs.extend(temp_traj[:,0])\n",
    "            ret = discounted_rewards(temp_traj[:,1], self.gamma)\n",
    "            self.returns.extend(ret)\n",
    "            self.actions.extend(temp_traj[:,2])\n",
    "        \n",
    "    def get_batch(self):\n",
    "        \n",
    "        return np.array(self.obs,dtype=np.float32), self.actions, self.returns\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        assert(len(self.obs) == len(self.actions) == len(self.returns))\n",
    "        return len(self.obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 8)\n",
      "[[ 0.00248661  1.4141665   0.2518405   0.14427547 -0.00287447 -0.05704566\n",
      "   0.          0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 8)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_test = gym.make('LunarLander-v2')\n",
    "buffer_test = Buffer(0.95)\n",
    "buffer_list = []\n",
    "\n",
    "obs = np.array([env_test.reset()])\n",
    "obs_2 = np.array([env_test.reset()])\n",
    "\n",
    "buffer_list.append([obs.copy(),10,1])\n",
    "buffer_list.append([obs_2.copy(),2,3])\n",
    "\n",
    "buffer_test.store(np.array(buffer_list))\n",
    "\n",
    "obs_buffer, action_buffer, return_buffer = buffer_test.get_batch()\n",
    "\n",
    "print(obs_buffer.shape)\n",
    "print(obs_buffer[1])\n",
    "obs_buffer[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REINFORCE(env_name, hidden_layers, hidden_size, activation, output_activation, \n",
    "              alpha, num_epochs, gamma, steps_per_epoch):\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.n\n",
    "    policy = Policy(hidden_layers, hidden_size, act_dim, activation, output_activation)\n",
    "    \n",
    "    obs = env.reset()\n",
    "    _ = policy.predict(np.array([obs]))\n",
    "    \n",
    "    policy.compile(optimizer = tf.keras.optimizers.Adam(alpha))\n",
    "    \n",
    "    step_count = 0\n",
    "    train_rewards = []\n",
    "    train_ep_len = []\n",
    "    \n",
    "    timer = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        obs = np.array([env.reset()])\n",
    "        buffer = Buffer(gamma)\n",
    "        env_buffer = []\n",
    "        epoch_rewards = []\n",
    "        \n",
    "        done = False\n",
    "        #while len(buffer) < steps_per_epoch:\n",
    "        while not done:\n",
    "            \n",
    "            policy_actions = policy.predict(obs)\n",
    "            action = tf.squeeze(tf.random.categorical(policy_actions,1))\n",
    "            next_obs, reward, done, _ = env.step(np.squeeze(action))\n",
    "            \n",
    "            env_buffer.append([obs.copy(), reward, action])\n",
    "            \n",
    "            obs = np.array([next_obs.copy()])\n",
    "            step_count += 1\n",
    "            epoch_rewards.append(reward)\n",
    "            \n",
    "            if done: \n",
    "                \n",
    "                buffer.store(np.array(env_buffer))\n",
    "                env_buffer = []\n",
    "                \n",
    "                train_rewards.append((np.sum(epoch_rewards)))\n",
    "                train_ep_len.append(len(epoch_rewards))\n",
    "                \n",
    "                obs = env.reset()\n",
    "                epoch_rewards = []\n",
    "                \n",
    "        # Policy Optimization\n",
    "        \n",
    "        obs_batch, action_batch, return_batch = buffer.get_batch()\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            one_hot_actions = tf.keras.utils.to_categorical(action_batch, act_dim, dtype=np.float32)\n",
    "            pi_logits = policy(obs_batch)\n",
    "            pi_log = tf.reduce_sum(tf.multiply(one_hot_actions.reshape(one_hot_actions.shape[0],1,one_hot_actions.shape[1]),\n",
    "                                               tf.nn.log_softmax(pi_logits)), axis=2)\n",
    "            \n",
    "            pi_loss = -tf.reduce_mean(pi_log * return_batch)\n",
    "            \n",
    "            model_gradients = tape.gradient(pi_loss, policy.trainable_variables)\n",
    "            policy.optimizer.apply_gradients(zip(model_gradients, policy.trainable_variables))\n",
    "        \n",
    "        # Statistics\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            \n",
    "            print('Ep:%d MnRew:%.2f MxRew:%.1f EpLen:%.1f Buffer:%d -- Step:%d -- Time:%d' % \n",
    "                  (epoch, np.mean(train_rewards), np.max(train_rewards), np.mean(train_ep_len), \n",
    "                   len(buffer), step_count,time.time()-timer))\n",
    "            \n",
    "            train_rewards = []\n",
    "            train_ep_len = []\n",
    "    \n",
    "    return env, policy\n",
    "    env.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot_actions.reshape([110,1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reduce_sum(tf.multiply(one_hot_actions.reshape(one_hot_actions.shape[0],1,one_hot_actions.shape[1]),\n",
    "                          #tf.nn.log_softmax(pi_logits)),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:0 MnRew:-357.07 MxRew:-357.1 EpLen:98.0 Buffer:98 -- Step:98 -- Time:1\n",
      "Ep:10 MnRew:-178.06 MxRew:-105.7 EpLen:84.4 Buffer:89 -- Step:942 -- Time:13\n",
      "Ep:20 MnRew:-114.64 MxRew:-72.0 EpLen:87.3 Buffer:78 -- Step:1815 -- Time:25\n",
      "Ep:30 MnRew:-158.02 MxRew:-63.0 EpLen:98.8 Buffer:108 -- Step:2803 -- Time:38\n",
      "Ep:40 MnRew:-161.47 MxRew:-48.3 EpLen:132.0 Buffer:257 -- Step:4123 -- Time:56\n",
      "Ep:50 MnRew:-340.19 MxRew:-5.4 EpLen:123.2 Buffer:94 -- Step:5355 -- Time:74\n",
      "Ep:60 MnRew:-218.40 MxRew:-14.7 EpLen:115.4 Buffer:105 -- Step:6509 -- Time:89\n",
      "Ep:70 MnRew:-222.05 MxRew:-1.7 EpLen:102.3 Buffer:81 -- Step:7532 -- Time:104\n",
      "Ep:80 MnRew:-100.43 MxRew:-13.7 EpLen:102.0 Buffer:72 -- Step:8552 -- Time:118\n",
      "Ep:90 MnRew:-97.97 MxRew:-62.8 EpLen:95.6 Buffer:78 -- Step:9508 -- Time:131\n",
      "Ep:100 MnRew:-105.37 MxRew:-26.4 EpLen:81.4 Buffer:70 -- Step:10322 -- Time:142\n",
      "Ep:110 MnRew:-91.61 MxRew:-60.0 EpLen:80.9 Buffer:80 -- Step:11131 -- Time:153\n",
      "Ep:120 MnRew:-105.84 MxRew:-67.4 EpLen:85.0 Buffer:66 -- Step:11981 -- Time:165\n",
      "Ep:130 MnRew:-110.74 MxRew:-66.3 EpLen:81.2 Buffer:130 -- Step:12793 -- Time:176\n",
      "Ep:140 MnRew:-109.18 MxRew:-35.6 EpLen:84.7 Buffer:55 -- Step:13640 -- Time:188\n",
      "Ep:150 MnRew:-153.38 MxRew:14.8 EpLen:71.5 Buffer:54 -- Step:14355 -- Time:198\n",
      "Ep:160 MnRew:-124.76 MxRew:-12.4 EpLen:74.2 Buffer:63 -- Step:15097 -- Time:208\n",
      "Ep:170 MnRew:-142.38 MxRew:-97.6 EpLen:74.2 Buffer:61 -- Step:15839 -- Time:218\n",
      "Ep:180 MnRew:-181.83 MxRew:17.2 EpLen:72.8 Buffer:66 -- Step:16567 -- Time:228\n",
      "Ep:190 MnRew:-239.45 MxRew:-162.1 EpLen:73.2 Buffer:72 -- Step:17299 -- Time:239\n",
      "Ep:200 MnRew:-292.01 MxRew:-133.7 EpLen:69.9 Buffer:56 -- Step:17998 -- Time:248\n",
      "Ep:210 MnRew:-178.66 MxRew:-89.9 EpLen:81.3 Buffer:96 -- Step:18811 -- Time:260\n",
      "Ep:220 MnRew:-94.96 MxRew:-49.4 EpLen:76.3 Buffer:91 -- Step:19574 -- Time:270\n",
      "Ep:230 MnRew:-111.88 MxRew:-67.7 EpLen:83.4 Buffer:95 -- Step:20408 -- Time:282\n",
      "Ep:240 MnRew:-65.76 MxRew:51.2 EpLen:94.0 Buffer:78 -- Step:21348 -- Time:295\n",
      "Ep:250 MnRew:-91.08 MxRew:-35.1 EpLen:92.8 Buffer:112 -- Step:22276 -- Time:308\n",
      "Ep:260 MnRew:-77.89 MxRew:11.6 EpLen:83.1 Buffer:74 -- Step:23107 -- Time:319\n",
      "Ep:270 MnRew:-85.09 MxRew:-33.8 EpLen:94.9 Buffer:101 -- Step:24056 -- Time:332\n",
      "Ep:280 MnRew:-87.79 MxRew:-6.0 EpLen:99.6 Buffer:81 -- Step:25052 -- Time:346\n",
      "Ep:290 MnRew:-65.73 MxRew:-14.5 EpLen:94.8 Buffer:118 -- Step:26000 -- Time:359\n",
      "Ep:300 MnRew:-76.75 MxRew:7.4 EpLen:101.5 Buffer:93 -- Step:27015 -- Time:374\n",
      "Ep:310 MnRew:-78.04 MxRew:4.6 EpLen:112.4 Buffer:107 -- Step:28139 -- Time:389\n",
      "Ep:320 MnRew:-89.78 MxRew:-21.5 EpLen:93.8 Buffer:114 -- Step:29077 -- Time:402\n",
      "Ep:330 MnRew:-52.93 MxRew:2.6 EpLen:91.0 Buffer:71 -- Step:29987 -- Time:415\n",
      "Ep:340 MnRew:-60.19 MxRew:-19.2 EpLen:81.7 Buffer:90 -- Step:30804 -- Time:426\n",
      "Ep:350 MnRew:-83.90 MxRew:-31.1 EpLen:92.4 Buffer:68 -- Step:31728 -- Time:439\n",
      "Ep:360 MnRew:-92.55 MxRew:-21.7 EpLen:106.3 Buffer:100 -- Step:32791 -- Time:454\n",
      "Ep:370 MnRew:-66.46 MxRew:-13.6 EpLen:84.5 Buffer:66 -- Step:33636 -- Time:466\n",
      "Ep:380 MnRew:-121.39 MxRew:-59.2 EpLen:72.6 Buffer:67 -- Step:34362 -- Time:476\n",
      "Ep:390 MnRew:-111.28 MxRew:-84.6 EpLen:75.0 Buffer:86 -- Step:35112 -- Time:486\n",
      "Ep:400 MnRew:-83.29 MxRew:29.3 EpLen:66.4 Buffer:54 -- Step:35776 -- Time:495\n",
      "Ep:410 MnRew:-114.44 MxRew:-12.7 EpLen:68.9 Buffer:76 -- Step:36465 -- Time:505\n",
      "Ep:420 MnRew:-121.14 MxRew:-82.1 EpLen:70.1 Buffer:83 -- Step:37166 -- Time:514\n",
      "Ep:430 MnRew:-113.86 MxRew:-45.6 EpLen:77.6 Buffer:98 -- Step:37942 -- Time:525\n",
      "Ep:440 MnRew:-85.20 MxRew:-28.0 EpLen:83.6 Buffer:75 -- Step:38778 -- Time:537\n",
      "Ep:450 MnRew:-58.99 MxRew:21.0 EpLen:75.5 Buffer:64 -- Step:39533 -- Time:547\n",
      "Ep:460 MnRew:-52.13 MxRew:7.1 EpLen:82.2 Buffer:91 -- Step:40355 -- Time:558\n",
      "Ep:470 MnRew:-57.81 MxRew:-6.2 EpLen:93.7 Buffer:104 -- Step:41292 -- Time:571\n",
      "Ep:480 MnRew:-47.20 MxRew:-1.4 EpLen:84.1 Buffer:111 -- Step:42133 -- Time:583\n",
      "Ep:490 MnRew:-20.79 MxRew:22.8 EpLen:86.9 Buffer:85 -- Step:43002 -- Time:595\n",
      "Ep:500 MnRew:-36.73 MxRew:11.8 EpLen:90.2 Buffer:84 -- Step:43904 -- Time:608\n",
      "Ep:510 MnRew:-73.38 MxRew:11.4 EpLen:99.4 Buffer:113 -- Step:44898 -- Time:621\n",
      "Ep:520 MnRew:-47.23 MxRew:28.1 EpLen:106.7 Buffer:128 -- Step:45965 -- Time:636\n",
      "Ep:530 MnRew:-23.42 MxRew:26.6 EpLen:112.3 Buffer:124 -- Step:47088 -- Time:651\n",
      "Ep:540 MnRew:-24.68 MxRew:90.3 EpLen:275.4 Buffer:85 -- Step:49842 -- Time:690\n",
      "Ep:550 MnRew:-34.56 MxRew:43.8 EpLen:102.1 Buffer:84 -- Step:50863 -- Time:704\n",
      "Ep:560 MnRew:-66.38 MxRew:-17.2 EpLen:90.9 Buffer:100 -- Step:51772 -- Time:716\n",
      "Ep:570 MnRew:-89.95 MxRew:-3.7 EpLen:112.2 Buffer:74 -- Step:52894 -- Time:732\n",
      "Ep:580 MnRew:-72.14 MxRew:148.8 EpLen:206.3 Buffer:1000 -- Step:54957 -- Time:760\n",
      "Ep:590 MnRew:-111.44 MxRew:12.5 EpLen:180.2 Buffer:113 -- Step:56759 -- Time:786\n",
      "Ep:600 MnRew:-71.81 MxRew:188.4 EpLen:202.3 Buffer:161 -- Step:58782 -- Time:814\n",
      "Ep:610 MnRew:-58.68 MxRew:299.0 EpLen:180.2 Buffer:156 -- Step:60584 -- Time:839\n",
      "Ep:620 MnRew:43.95 MxRew:293.0 EpLen:246.0 Buffer:181 -- Step:63044 -- Time:873\n",
      "Ep:630 MnRew:-18.57 MxRew:194.1 EpLen:299.5 Buffer:378 -- Step:66039 -- Time:915\n",
      "Ep:640 MnRew:-3.66 MxRew:303.1 EpLen:377.4 Buffer:289 -- Step:69813 -- Time:968\n",
      "Ep:650 MnRew:-43.56 MxRew:198.6 EpLen:450.2 Buffer:358 -- Step:74315 -- Time:1032\n",
      "Ep:660 MnRew:-16.07 MxRew:228.8 EpLen:315.4 Buffer:299 -- Step:77469 -- Time:1077\n",
      "Ep:670 MnRew:21.25 MxRew:260.3 EpLen:344.2 Buffer:133 -- Step:80911 -- Time:1126\n",
      "Ep:680 MnRew:74.41 MxRew:269.3 EpLen:291.9 Buffer:127 -- Step:83830 -- Time:1167\n",
      "Ep:690 MnRew:24.33 MxRew:170.4 EpLen:240.7 Buffer:162 -- Step:86237 -- Time:1200\n",
      "Ep:700 MnRew:94.57 MxRew:243.4 EpLen:531.2 Buffer:1000 -- Step:91549 -- Time:1276\n",
      "Ep:710 MnRew:17.90 MxRew:129.8 EpLen:211.7 Buffer:201 -- Step:93666 -- Time:1305\n",
      "Ep:720 MnRew:81.31 MxRew:300.7 EpLen:252.7 Buffer:113 -- Step:96193 -- Time:1341\n",
      "Ep:730 MnRew:41.32 MxRew:197.5 EpLen:241.6 Buffer:169 -- Step:98609 -- Time:1374\n",
      "Ep:740 MnRew:27.18 MxRew:189.6 EpLen:223.8 Buffer:143 -- Step:100847 -- Time:1406\n",
      "Ep:750 MnRew:5.34 MxRew:39.7 EpLen:138.7 Buffer:161 -- Step:102234 -- Time:1425\n",
      "Ep:760 MnRew:74.60 MxRew:174.0 EpLen:571.2 Buffer:164 -- Step:107946 -- Time:1507\n",
      "Ep:770 MnRew:32.83 MxRew:160.7 EpLen:328.0 Buffer:1000 -- Step:111226 -- Time:1553\n",
      "Ep:780 MnRew:54.85 MxRew:159.6 EpLen:395.1 Buffer:148 -- Step:115177 -- Time:1609\n",
      "Ep:790 MnRew:20.85 MxRew:170.5 EpLen:236.3 Buffer:161 -- Step:117540 -- Time:1643\n",
      "Ep:800 MnRew:39.44 MxRew:171.9 EpLen:341.1 Buffer:172 -- Step:120951 -- Time:1691\n",
      "Ep:810 MnRew:46.22 MxRew:167.5 EpLen:320.3 Buffer:152 -- Step:124154 -- Time:1736\n",
      "Ep:820 MnRew:50.85 MxRew:142.9 EpLen:595.6 Buffer:226 -- Step:130110 -- Time:1822\n",
      "Ep:830 MnRew:-7.49 MxRew:138.7 EpLen:459.5 Buffer:1000 -- Step:134705 -- Time:1888\n",
      "Ep:840 MnRew:79.63 MxRew:224.8 EpLen:642.4 Buffer:1000 -- Step:141129 -- Time:1981\n",
      "Ep:850 MnRew:62.45 MxRew:231.4 EpLen:541.2 Buffer:163 -- Step:146541 -- Time:2058\n",
      "Ep:860 MnRew:-30.93 MxRew:37.8 EpLen:249.6 Buffer:208 -- Step:149037 -- Time:2094\n",
      "Ep:870 MnRew:38.28 MxRew:177.2 EpLen:563.6 Buffer:1000 -- Step:154673 -- Time:2174\n",
      "Ep:880 MnRew:7.77 MxRew:180.0 EpLen:491.5 Buffer:238 -- Step:159588 -- Time:2246\n",
      "Ep:890 MnRew:-16.51 MxRew:129.7 EpLen:535.9 Buffer:227 -- Step:164947 -- Time:2324\n",
      "Ep:900 MnRew:32.57 MxRew:187.6 EpLen:392.7 Buffer:221 -- Step:168874 -- Time:2381\n",
      "Ep:910 MnRew:54.05 MxRew:258.9 EpLen:614.2 Buffer:178 -- Step:175016 -- Time:2469\n",
      "Ep:920 MnRew:9.43 MxRew:142.4 EpLen:588.5 Buffer:212 -- Step:180901 -- Time:2555\n",
      "Ep:930 MnRew:75.23 MxRew:225.6 EpLen:604.9 Buffer:251 -- Step:186950 -- Time:2642\n",
      "Ep:940 MnRew:103.06 MxRew:189.4 EpLen:732.3 Buffer:1000 -- Step:194273 -- Time:2756\n",
      "Ep:950 MnRew:59.01 MxRew:156.8 EpLen:564.5 Buffer:198 -- Step:199918 -- Time:2844\n",
      "Ep:960 MnRew:49.54 MxRew:118.0 EpLen:634.3 Buffer:1000 -- Step:206261 -- Time:2940\n",
      "Ep:970 MnRew:88.04 MxRew:165.7 EpLen:858.3 Buffer:1000 -- Step:214844 -- Time:3069\n",
      "Ep:980 MnRew:54.98 MxRew:193.9 EpLen:706.0 Buffer:1000 -- Step:221904 -- Time:3177\n",
      "Ep:990 MnRew:84.04 MxRew:137.7 EpLen:985.2 Buffer:1000 -- Step:231756 -- Time:3325\n"
     ]
    }
   ],
   "source": [
    "#def REINFORCE(env_name, hidden_layers, hidden_size, activation, output_activation, \n",
    "              #alpha, num_epochs, gamma, steps_per_epoch):\n",
    "    \n",
    "env,policy = REINFORCE('LunarLander-v2', 1, [64], activation=tf.tanh, output_activation=None, \n",
    "              alpha=8e-3, num_epochs=1000, gamma=0.99, steps_per_epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "env_name = 'LunarLander-v2'\n",
    "env_test = gym.make(env_name)\n",
    "\n",
    "#env_test = gym.wrappers.Monitor(env_test, \"VIDEOS/TEST_VIDEOS\"+env_name+str(current_milli_time()), force=True,\n",
    " #                                   video_callable=lambda x: x%20==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test.close()\n",
    "obs = env_test.reset()\n",
    "obs = np.array([obs])\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    policy_actions = policy.predict(obs)\n",
    "    action = tf.squeeze(tf.random.categorical(policy_actions,1))\n",
    "    next_obs, reward, done, _ = env_test.step(np.squeeze(action))\n",
    "    env_test.render()\n",
    "            \n",
    "    obs = np.array([next_obs.copy()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
